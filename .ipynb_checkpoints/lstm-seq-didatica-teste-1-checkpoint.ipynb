{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/giovannafilipakis/Desktop/keras-projects/database'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b4e6b53998d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m##### Data in text format #####\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Read, then decode for py2 compat.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/Users/giovannafilipakis/Desktop/keras-projects/database'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# length of text is the number of characters in it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/giovannafilipakis/Desktop/keras-projects/database'"
     ]
    }
   ],
   "source": [
    "##### Data in text format #####\n",
    "# Read, then decode for py2 compat.\n",
    "text = open('/Users/giovannafilipakis/Desktop/keras-projects/database', 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-b07a9f2383e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#qtnd de caracteres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mvocab_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'{} unique characters'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "##### Data in text format #####\n",
    "#qtnd de caracteres \n",
    "\n",
    "vocab_t = sorted(set(text))\n",
    "print(vocab_t)\n",
    "print ('{} unique characters'.format(len(vocab_t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Data in list format #####\n",
    "\n",
    "data_set = ['Lista', 'Pilha', 'Fila', 'Lista Simplesmente Encadeada', 'Lista Duplamente Encadeada', 'Árvore', '9',\n",
    "             'Pilha', 'Fila', 'Lista', 'Lista Simplesmente Encadeada', 'Lista Duplamente Encadeada', 'Árvore', '6',\n",
    "             'Árvore', 'Pilha', 'Fila', 'Lista', 'Lista Simplesmente Encadeada', 'Lista Duplamente Encadeada', '5',\n",
    "             'Pilha', 'Lista', 'Lista Simplesmente Encadeada', 'Lista Duplamente Encadeada', 'Fila', 'Árvore', '6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', '6', '9', 'Fila', 'Lista', 'Lista Duplamente Encadeada', 'Lista Simplesmente Encadeada', 'Pilha', 'Árvore']\n",
      "9 unique characters\n"
     ]
    }
   ],
   "source": [
    "##### Data in list format #####\n",
    "#qtnd de caracteres \n",
    "\n",
    "vocab_l = sorted(set(data_set))\n",
    "print(vocab_l)\n",
    "print ('{} unique characters'.format(len(vocab_l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '5' :   0,\n",
      "  '6' :   1,\n",
      "  '9' :   2,\n",
      "  'Fila':   3,\n",
      "  'Lista':   4,\n",
      "  'Lista Duplamente Encadeada':   5,\n",
      "  'Lista Simplesmente Encadeada':   6,\n",
      "  'Pilha':   7,\n",
      "  'Árvore':   8,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "\n",
    "char2idx = {u:i for i, u in enumerate(vocab_l)}\n",
    "idx2char = np.array(vocab_l)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in data_set])\n",
    "\n",
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lista' 'Pilha' 'Fila' 'Lista Simplesmente Encadeada'\n",
      " 'Lista Duplamente Encadeada' 'Árvore' '9' 'Pilha' 'Fila' 'Lista'\n",
      " 'Lista Simplesmente Encadeada' 'Lista Duplamente Encadeada' 'Árvore' '6'\n",
      " 'Árvore' 'Pilha' 'Fila' 'Lista' 'Lista Simplesmente Encadeada'\n",
      " 'Lista Duplamente Encadeada' '5' 'Pilha' 'Lista'\n",
      " 'Lista Simplesmente Encadeada' 'Lista Duplamente Encadeada' 'Fila'\n",
      " 'Árvore' '6']\n",
      "[4 7 3 6 5 8 2 7 3 4 6 5 8 1 8 7 3 4 6 5 0 7 4 6 5 3 8 1]\n"
     ]
    }
   ],
   "source": [
    "#char2int das sequências\n",
    "\n",
    "values = np.array(data_set)\n",
    "print(values)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#one-hot encoding das sequências\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lista']\n",
      "['Pilha']\n",
      "['Fila']\n",
      "['Lista Simplesmente Encadeada']\n",
      "['Lista Duplamente Encadeada']\n",
      "['Árvore']\n",
      "['9']\n",
      "['Pilha']\n",
      "['Fila']\n",
      "['Lista']\n",
      "['Lista Simplesmente Encadeada']\n",
      "['Lista Duplamente Encadeada']\n",
      "['Árvore']\n",
      "['6']\n",
      "['Árvore']\n",
      "['Pilha']\n",
      "['Fila']\n",
      "['Lista']\n",
      "['Lista Simplesmente Encadeada']\n",
      "['Lista Duplamente Encadeada']\n",
      "['5']\n",
      "['Pilha']\n",
      "['Lista']\n",
      "['Lista Simplesmente Encadeada']\n",
      "['Lista Duplamente Encadeada']\n",
      "['Fila']\n",
      "['Árvore']\n",
      "['6']\n"
     ]
    }
   ],
   "source": [
    "#reverter one-hot encoding das sequências\n",
    "\n",
    "for i in onehot_encoded:\n",
    "    inverted = label_encoder.inverse_transform([np.argmax(i)])\n",
    "    print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]], shape=(7, 9), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]], shape=(7, 9), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(7, 9), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]], shape=(7, 9), dtype=float64)\n",
      "Lista\n",
      "Pilha\n",
      "Fila\n",
      "Lista Simplesmente Encadeada\n",
      "Lista Duplamente Encadeada\n",
      "Árvore\n",
      "9\n",
      "Pilha\n",
      "Fila\n",
      "Lista\n",
      "Lista Simplesmente Encadeada\n",
      "Lista Duplamente Encadeada\n",
      "Árvore\n",
      "6\n",
      "Árvore\n",
      "Pilha\n",
      "Fila\n",
      "Lista\n",
      "Lista Simplesmente Encadeada\n",
      "Lista Duplamente Encadeada\n",
      "5\n",
      "Pilha\n",
      "Lista\n",
      "Lista Simplesmente Encadeada\n",
      "Lista Duplamente Encadeada\n",
      "Fila\n",
      "Árvore\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "##### Create training examples and targets #####\n",
    "\n",
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 6\n",
    "examples_per_epoch = len(data_set)#(seq_length+1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(onehot_encoded)\n",
    "# for i in char_dataset:\n",
    "#     print(i)\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "#print batches encoded\n",
    "for i in sequences:\n",
    "    print(i)\n",
    "    \n",
    "#print batches as text\n",
    "for i in sequences:\n",
    "    for j in i:\n",
    "        print(''.join(label_encoder.inverse_transform([np.argmax(j)])))\n",
    "\n",
    "# for item in sequences:\n",
    "#   print(repr(''.join(label_encoder.inverse_transform([np.argmax(item)]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista\n",
      "Pilha\n",
      "Fila\n",
      "Lista Simplesmente Encadeada\n",
      "Lista Duplamente Encadeada\n",
      "Árvore\n",
      "\n",
      "\n",
      "Pilha\n",
      "Fila\n",
      "Lista Simplesmente Encadeada\n",
      "Lista Duplamente Encadeada\n",
      "Árvore\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "##### Create training examples and targets #####\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "#foreach item in sequences apply split_input_target\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "#print input_text and target_text as text\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    for i in input_example:\n",
    "        print(''.join(label_encoder.inverse_transform([np.argmax(i)])))\n",
    "    print('\\n')\n",
    "    for i in target_example:\n",
    "        print(''.join(label_encoder.inverse_transform([np.argmax(i)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: [0. 0. 0. 0. 1. 0. 0. 0. 0.] (['Lista'])\n",
      "  expected output: [0. 0. 0. 0. 0. 0. 0. 1. 0.] (['Pilha'])\n",
      "Step    1\n",
      "  input: [0. 0. 0. 0. 0. 0. 0. 1. 0.] (['Pilha'])\n",
      "  expected output: [0. 0. 0. 1. 0. 0. 0. 0. 0.] (['Fila'])\n",
      "Step    2\n",
      "  input: [0. 0. 0. 1. 0. 0. 0. 0. 0.] (['Fila'])\n",
      "  expected output: [0. 0. 0. 0. 0. 0. 1. 0. 0.] (['Lista Simplesmente Encadeada'])\n",
      "Step    3\n",
      "  input: [0. 0. 0. 0. 0. 0. 1. 0. 0.] (['Lista Simplesmente Encadeada'])\n",
      "  expected output: [0. 0. 0. 0. 0. 1. 0. 0. 0.] (['Lista Duplamente Encadeada'])\n",
      "Step    4\n",
      "  input: [0. 0. 0. 0. 0. 1. 0. 0. 0.] (['Lista Duplamente Encadeada'])\n",
      "  expected output: [0. 0. 0. 0. 0. 0. 0. 0. 1.] (['Árvore'])\n",
      "Step    5\n",
      "  input: [0. 0. 0. 0. 0. 0. 0. 0. 1.] (['Árvore'])\n",
      "  expected output: [0. 0. 1. 0. 0. 0. 0. 0. 0.] (['9'])\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example, target_example)):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, str(label_encoder.inverse_transform([np.argmax(input_idx)]))))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, str(label_encoder.inverse_transform([np.argmax(target_idx)]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((1, 6, 9), (1, 6, 9)), types: (tf.float64, tf.float64)>\n",
      "(<tf.Tensor: shape=(1, 6, 9), dtype=float64, numpy=\n",
      "array([[[0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.]]])>, <tf.Tensor: shape=(1, 6, 9), dtype=float64, numpy=\n",
      "array([[[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0.]]])>)\n",
      "(<tf.Tensor: shape=(1, 6, 9), dtype=float64, numpy=\n",
      "array([[[0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.]]])>, <tf.Tensor: shape=(1, 6, 9), dtype=float64, numpy=\n",
      "array([[[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0.]]])>)\n",
      "(<tf.Tensor: shape=(1, 6, 9), dtype=float64, numpy=\n",
      "array([[[0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.]]])>, <tf.Tensor: shape=(1, 6, 9), dtype=float64, numpy=\n",
      "array([[[0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0.]]])>)\n",
      "(<tf.Tensor: shape=(1, 6, 9), dtype=float64, numpy=\n",
      "array([[[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.]]])>, <tf.Tensor: shape=(1, 6, 9), dtype=float64, numpy=\n",
      "array([[[0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]]])>)\n"
     ]
    }
   ],
   "source": [
    "##### Create training batches #####\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10\n",
    "\n",
    "# print(dataset)\n",
    "# for i in dataset:\n",
    "#     print(i)\n",
    "\n",
    "datasetBatch = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "# If your program depends on the batches having the same outer dimension,\n",
    "# you should set the drop_remainder argument to True to prevent the smaller\n",
    "# batch from being produced.\n",
    "\n",
    "print(datasetBatch)\n",
    "for i in datasetBatch:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Building the model #####\n",
    "\n",
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab_l)\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "def build_model(vocab_size, rnn_units):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(rnn_units,batch_input_shape=(1, 6, 9)),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "    vocab_size = len(vocab_l),\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.00132412  0.00308277 -0.01120764 -0.01083153 -0.00308167 -0.00348565\n",
      "  -0.00337448  0.00788209 -0.00353483]], shape=(1, 9), dtype=float32) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "##### Testing the model #####\n",
    "\n",
    "for input_example_batch, target_example_batch in datasetBatch.take(1):\n",
    "    #print('input',input_example_batch)\n",
    "    #print()\n",
    "    #print('target',target_example_batch)\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions, \"# (batch_size, sequence_length, vocab_size)\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (1, 1024)                 4235264   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, 9)                    9225      \n",
      "=================================================================\n",
      "Total params: 4,244,489\n",
      "Trainable params: 4,244,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    " model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
