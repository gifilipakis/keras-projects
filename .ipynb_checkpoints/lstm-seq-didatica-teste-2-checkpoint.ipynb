{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Data in list format #####\n",
    "\n",
    "\n",
    "#Todas as sequências tem o mesmo comprimento\n",
    "#Vetores de tamanho fixo\n",
    "data_set = ['Lista', 'Pilha', 'Fila', 'Lista Simplesmente Encadeada', 'Lista Duplamente Encadeada', 'Árvore', '9',\n",
    "             'Pilha', 'Fila', 'Lista', 'Lista Simplesmente Encadeada', 'Lista Duplamente Encadeada', 'Árvore', '6',\n",
    "             'Árvore', 'Pilha', 'Fila', 'Lista', 'Lista Simplesmente Encadeada', 'Lista Duplamente Encadeada', '5',\n",
    "             'Pilha', 'Lista', 'Lista Simplesmente Encadeada', 'Lista Duplamente Encadeada', 'Fila', 'Árvore', '6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', '6', '9', 'Fila', 'Lista', 'Lista Duplamente Encadeada', 'Lista Simplesmente Encadeada', 'Pilha', 'Árvore']\n",
      "9 itens distintos\n"
     ]
    }
   ],
   "source": [
    "##### Data in list format #####\n",
    "\n",
    "#itens distintos \n",
    "vocab_l = sorted(set(data_set))\n",
    "print(vocab_l)\n",
    "print ('{} itens distintos'.format(len(vocab_l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '5' :   0,\n",
      "  '6' :   1,\n",
      "  '9' :   2,\n",
      "  'Fila':   3,\n",
      "  'Lista':   4,\n",
      "  'Lista Duplamente Encadeada':   5,\n",
      "  'Lista Simplesmente Encadeada':   6,\n",
      "  'Pilha':   7,\n",
      "  'Árvore':   8,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "\n",
    "char2idx = {u:i for i, u in enumerate(vocab_l)}\n",
    "idx2char = np.array(vocab_l)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in data_set])\n",
    "\n",
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lista' 'Pilha' 'Fila' 'Lista Simplesmente Encadeada'\n",
      " 'Lista Duplamente Encadeada' 'Árvore' '9' 'Pilha' 'Fila' 'Lista'\n",
      " 'Lista Simplesmente Encadeada' 'Lista Duplamente Encadeada' 'Árvore' '6'\n",
      " 'Árvore' 'Pilha' 'Fila' 'Lista' 'Lista Simplesmente Encadeada'\n",
      " 'Lista Duplamente Encadeada' '5' 'Pilha' 'Lista'\n",
      " 'Lista Simplesmente Encadeada' 'Lista Duplamente Encadeada' 'Fila'\n",
      " 'Árvore' '6']\n",
      "[4 7 3 6 5 8 2 7 3 4 6 5 8 1 8 7 3 4 6 5 0 7 4 6 5 3 8 1]\n"
     ]
    }
   ],
   "source": [
    "#char2int das sequências\n",
    "\n",
    "values = np.array(data_set)\n",
    "print(values)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#one-hot encoding das sequências\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lista']\n",
      "['Pilha']\n",
      "['Fila']\n",
      "['Lista Simplesmente Encadeada']\n",
      "['Lista Duplamente Encadeada']\n",
      "['Árvore']\n",
      "['9']\n",
      "['Pilha']\n",
      "['Fila']\n",
      "['Lista']\n",
      "['Lista Simplesmente Encadeada']\n",
      "['Lista Duplamente Encadeada']\n",
      "['Árvore']\n",
      "['6']\n",
      "['Árvore']\n",
      "['Pilha']\n",
      "['Fila']\n",
      "['Lista']\n",
      "['Lista Simplesmente Encadeada']\n",
      "['Lista Duplamente Encadeada']\n",
      "['5']\n",
      "['Pilha']\n",
      "['Lista']\n",
      "['Lista Simplesmente Encadeada']\n",
      "['Lista Duplamente Encadeada']\n",
      "['Fila']\n",
      "['Árvore']\n",
      "['6']\n"
     ]
    }
   ],
   "source": [
    "#reverter one-hot encoding das sequências\n",
    "\n",
    "for i in onehot_encoded:\n",
    "    inverted = label_encoder.inverse_transform([np.argmax(i)])\n",
    "    print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]], shape=(7, 9), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]], shape=(7, 9), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(7, 9), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]], shape=(7, 9), dtype=float64)\n",
      "Lista\n",
      "Pilha\n",
      "Fila\n",
      "Lista Simplesmente Encadeada\n",
      "Lista Duplamente Encadeada\n",
      "Árvore\n",
      "9\n",
      "Pilha\n",
      "Fila\n",
      "Lista\n",
      "Lista Simplesmente Encadeada\n",
      "Lista Duplamente Encadeada\n",
      "Árvore\n",
      "6\n",
      "Árvore\n",
      "Pilha\n",
      "Fila\n",
      "Lista\n",
      "Lista Simplesmente Encadeada\n",
      "Lista Duplamente Encadeada\n",
      "5\n",
      "Pilha\n",
      "Lista\n",
      "Lista Simplesmente Encadeada\n",
      "Lista Duplamente Encadeada\n",
      "Fila\n",
      "Árvore\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "##### Create training examples and targets #####\n",
    "\n",
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 6\n",
    "examples_per_epoch = len(data_set)#(seq_length+1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(onehot_encoded)\n",
    "# for i in char_dataset:\n",
    "#     print(i)\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "#print batches encoded\n",
    "for i in sequences:\n",
    "    print(i)\n",
    "    \n",
    "#print batches as text\n",
    "for i in sequences:\n",
    "    for j in i:\n",
    "        print(''.join(label_encoder.inverse_transform([np.argmax(j)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Input-----\n",
      "Lista\n",
      "Pilha\n",
      "Fila\n",
      "Lista Simplesmente Encadeada\n",
      "Lista Duplamente Encadeada\n",
      "Árvore\n",
      "-----Target-----\n",
      "Pilha\n",
      "Fila\n",
      "Lista Simplesmente Encadeada\n",
      "Lista Duplamente Encadeada\n",
      "Árvore\n",
      "9\n",
      "-----Input-----\n",
      "Pilha\n",
      "Fila\n",
      "Lista\n",
      "Lista Simplesmente Encadeada\n",
      "Lista Duplamente Encadeada\n",
      "Árvore\n",
      "-----Target-----\n",
      "Fila\n",
      "Lista\n",
      "Lista Simplesmente Encadeada\n",
      "Lista Duplamente Encadeada\n",
      "Árvore\n",
      "6\n",
      "-----Input-----\n",
      "Árvore\n",
      "Pilha\n",
      "Fila\n",
      "Lista\n",
      "Lista Simplesmente Encadeada\n",
      "Lista Duplamente Encadeada\n",
      "-----Target-----\n",
      "Pilha\n",
      "Fila\n",
      "Lista\n",
      "Lista Simplesmente Encadeada\n",
      "Lista Duplamente Encadeada\n",
      "5\n",
      "-----Input-----\n",
      "Pilha\n",
      "Lista\n",
      "Lista Simplesmente Encadeada\n",
      "Lista Duplamente Encadeada\n",
      "Fila\n",
      "Árvore\n",
      "-----Target-----\n",
      "Lista\n",
      "Lista Simplesmente Encadeada\n",
      "Lista Duplamente Encadeada\n",
      "Fila\n",
      "Árvore\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "##### Create training examples and targets #####\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "#foreach item in sequences apply split_input_target\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "#print input_text and target_text as text\n",
    "#não necessário, para visualização\n",
    "for input_example, target_example in  dataset:\n",
    "    print('-----Input-----')\n",
    "    for i in input_example:\n",
    "        print(''.join(label_encoder.inverse_transform([np.argmax(i)])))\n",
    "    print('-----Target-----')\n",
    "    for i in target_example:\n",
    "        print(''.join(label_encoder.inverse_transform([np.argmax(i)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: [0. 0. 0. 0. 1. 0. 0. 0. 0.] (['Lista'])\n",
      "  expected output: [0. 0. 0. 0. 0. 0. 0. 1. 0.] (['Pilha'])\n",
      "Step    1\n",
      "  input: [0. 0. 0. 0. 0. 0. 0. 1. 0.] (['Pilha'])\n",
      "  expected output: [0. 0. 0. 1. 0. 0. 0. 0. 0.] (['Fila'])\n",
      "Step    2\n",
      "  input: [0. 0. 0. 1. 0. 0. 0. 0. 0.] (['Fila'])\n",
      "  expected output: [0. 0. 0. 0. 0. 0. 1. 0. 0.] (['Lista Simplesmente Encadeada'])\n",
      "Step    3\n",
      "  input: [0. 0. 0. 0. 0. 0. 1. 0. 0.] (['Lista Simplesmente Encadeada'])\n",
      "  expected output: [0. 0. 0. 0. 0. 1. 0. 0. 0.] (['Lista Duplamente Encadeada'])\n",
      "Step    4\n",
      "  input: [0. 0. 0. 0. 0. 1. 0. 0. 0.] (['Lista Duplamente Encadeada'])\n",
      "  expected output: [0. 0. 0. 0. 0. 0. 0. 0. 1.] (['Árvore'])\n",
      "Step    5\n",
      "  input: [0. 0. 0. 0. 0. 0. 0. 0. 1.] (['Árvore'])\n",
      "  expected output: [0. 0. 1. 0. 0. 0. 0. 0. 0.] (['9'])\n",
      "------------------------------------------------------------------\n",
      "Step    0\n",
      "  input: [0. 0. 0. 0. 0. 0. 0. 1. 0.] (['Pilha'])\n",
      "  expected output: [0. 0. 0. 1. 0. 0. 0. 0. 0.] (['Fila'])\n",
      "Step    1\n",
      "  input: [0. 0. 0. 1. 0. 0. 0. 0. 0.] (['Fila'])\n",
      "  expected output: [0. 0. 0. 0. 1. 0. 0. 0. 0.] (['Lista'])\n",
      "Step    2\n",
      "  input: [0. 0. 0. 0. 1. 0. 0. 0. 0.] (['Lista'])\n",
      "  expected output: [0. 0. 0. 0. 0. 0. 1. 0. 0.] (['Lista Simplesmente Encadeada'])\n",
      "Step    3\n",
      "  input: [0. 0. 0. 0. 0. 0. 1. 0. 0.] (['Lista Simplesmente Encadeada'])\n",
      "  expected output: [0. 0. 0. 0. 0. 1. 0. 0. 0.] (['Lista Duplamente Encadeada'])\n",
      "Step    4\n",
      "  input: [0. 0. 0. 0. 0. 1. 0. 0. 0.] (['Lista Duplamente Encadeada'])\n",
      "  expected output: [0. 0. 0. 0. 0. 0. 0. 0. 1.] (['Árvore'])\n",
      "Step    5\n",
      "  input: [0. 0. 0. 0. 0. 0. 0. 0. 1.] (['Árvore'])\n",
      "  expected output: [0. 1. 0. 0. 0. 0. 0. 0. 0.] (['6'])\n",
      "------------------------------------------------------------------\n",
      "Step    0\n",
      "  input: [0. 0. 0. 0. 0. 0. 0. 0. 1.] (['Árvore'])\n",
      "  expected output: [0. 0. 0. 0. 0. 0. 0. 1. 0.] (['Pilha'])\n",
      "Step    1\n",
      "  input: [0. 0. 0. 0. 0. 0. 0. 1. 0.] (['Pilha'])\n",
      "  expected output: [0. 0. 0. 1. 0. 0. 0. 0. 0.] (['Fila'])\n",
      "Step    2\n",
      "  input: [0. 0. 0. 1. 0. 0. 0. 0. 0.] (['Fila'])\n",
      "  expected output: [0. 0. 0. 0. 1. 0. 0. 0. 0.] (['Lista'])\n",
      "Step    3\n",
      "  input: [0. 0. 0. 0. 1. 0. 0. 0. 0.] (['Lista'])\n",
      "  expected output: [0. 0. 0. 0. 0. 0. 1. 0. 0.] (['Lista Simplesmente Encadeada'])\n",
      "Step    4\n",
      "  input: [0. 0. 0. 0. 0. 0. 1. 0. 0.] (['Lista Simplesmente Encadeada'])\n",
      "  expected output: [0. 0. 0. 0. 0. 1. 0. 0. 0.] (['Lista Duplamente Encadeada'])\n",
      "Step    5\n",
      "  input: [0. 0. 0. 0. 0. 1. 0. 0. 0.] (['Lista Duplamente Encadeada'])\n",
      "  expected output: [1. 0. 0. 0. 0. 0. 0. 0. 0.] (['5'])\n",
      "------------------------------------------------------------------\n",
      "Step    0\n",
      "  input: [0. 0. 0. 0. 0. 0. 0. 1. 0.] (['Pilha'])\n",
      "  expected output: [0. 0. 0. 0. 1. 0. 0. 0. 0.] (['Lista'])\n",
      "Step    1\n",
      "  input: [0. 0. 0. 0. 1. 0. 0. 0. 0.] (['Lista'])\n",
      "  expected output: [0. 0. 0. 0. 0. 0. 1. 0. 0.] (['Lista Simplesmente Encadeada'])\n",
      "Step    2\n",
      "  input: [0. 0. 0. 0. 0. 0. 1. 0. 0.] (['Lista Simplesmente Encadeada'])\n",
      "  expected output: [0. 0. 0. 0. 0. 1. 0. 0. 0.] (['Lista Duplamente Encadeada'])\n",
      "Step    3\n",
      "  input: [0. 0. 0. 0. 0. 1. 0. 0. 0.] (['Lista Duplamente Encadeada'])\n",
      "  expected output: [0. 0. 0. 1. 0. 0. 0. 0. 0.] (['Fila'])\n",
      "Step    4\n",
      "  input: [0. 0. 0. 1. 0. 0. 0. 0. 0.] (['Fila'])\n",
      "  expected output: [0. 0. 0. 0. 0. 0. 0. 0. 1.] (['Árvore'])\n",
      "Step    5\n",
      "  input: [0. 0. 0. 0. 0. 0. 0. 0. 1.] (['Árvore'])\n",
      "  expected output: [0. 1. 0. 0. 0. 0. 0. 0. 0.] (['6'])\n",
      "------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#não necessário, para visualização\n",
    "for input_example, target_example in  dataset:\n",
    "    for i, (input_idx, target_idx) in enumerate(zip(input_example, target_example)):\n",
    "        print(\"Step {:4d}\".format(i))\n",
    "        print(\"  input: {} ({:s})\".format(input_idx, str(label_encoder.inverse_transform([np.argmax(input_idx)]))))\n",
    "        print(\"  expected output: {} ({:s})\".format(target_idx, str(label_encoder.inverse_transform([np.argmax(target_idx)]))))\n",
    "    print('------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((1, 6, 9), (1, 6, 9)), types: (tf.float64, tf.float64)>\n",
      "(<tf.Tensor: shape=(1, 6, 9), dtype=float64, numpy=\n",
      "array([[[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.]]])>, <tf.Tensor: shape=(1, 6, 9), dtype=float64, numpy=\n",
      "array([[[0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.]]])>)\n",
      "(<tf.Tensor: shape=(1, 6, 9), dtype=float64, numpy=\n",
      "array([[[0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.]]])>, <tf.Tensor: shape=(1, 6, 9), dtype=float64, numpy=\n",
      "array([[[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0.]]])>)\n",
      "(<tf.Tensor: shape=(1, 6, 9), dtype=float64, numpy=\n",
      "array([[[0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.]]])>, <tf.Tensor: shape=(1, 6, 9), dtype=float64, numpy=\n",
      "array([[[0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0.]]])>)\n",
      "(<tf.Tensor: shape=(1, 6, 9), dtype=float64, numpy=\n",
      "array([[[0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.]]])>, <tf.Tensor: shape=(1, 6, 9), dtype=float64, numpy=\n",
      "array([[[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0.]]])>)\n"
     ]
    }
   ],
   "source": [
    "##### Create training batches #####\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10\n",
    "\n",
    "# print(dataset)\n",
    "# for i in dataset:\n",
    "#     print(i)\n",
    "\n",
    "datasetBatch = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "# If your program depends on the batches having the same outer dimension,\n",
    "# you should set the drop_remainder argument to True to prevent the smaller\n",
    "# batch from being produced.\n",
    "\n",
    "print(datasetBatch)\n",
    "for i in datasetBatch:\n",
    "    print(i)\n",
    "\n",
    "\n",
    "#for i in datasetBatch.take(1):\n",
    "#    for j in i:\n",
    "#        for a in j:\n",
    "#            for e in a:\n",
    "#                print(''.join(label_encoder.inverse_transform([np.argmax(e)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Building the model #####\n",
    "\n",
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab_l)\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "def build_model(vocab_size, rnn_units):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(rnn_units,batch_input_shape=(1, 6, 9)),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "    vocab_size = len(vocab_l),\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (1, 1024)                 4235264   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (1, 9)                    9225      \n",
      "=================================================================\n",
      "Total params: 4,244,489\n",
      "Trainable params: 4,244,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    " model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tf.Tensor(\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1.]]], shape=(1, 6, 9), dtype=float64)\n",
      "\n",
      "Target: tf.Tensor(\n",
      "[[[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0.]]], shape=(1, 6, 9), dtype=float64)\n",
      "\n",
      "Batche Predictions: tf.Tensor(\n",
      "[[ 0.01813579 -0.0004243  -0.00978522  0.0186614   0.01369935 -0.00464518\n",
      "   0.01937051 -0.00422775 -0.00045989]], shape=(1, 9), dtype=float32) # (batch_size, sequence_length, vocab_size)\n",
      "\n",
      "Sample Indices: [8]\n",
      "--------------------------------------------------------\n",
      "Input: tf.Tensor(\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0.]]], shape=(1, 6, 9), dtype=float64)\n",
      "\n",
      "Target: tf.Tensor(\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0.]]], shape=(1, 6, 9), dtype=float64)\n",
      "\n",
      "Batche Predictions: tf.Tensor(\n",
      "[[ 0.01222751  0.00125837 -0.02271434  0.02449252  0.00365537  0.00315456\n",
      "   0.01286753 -0.00435885  0.00903745]], shape=(1, 9), dtype=float32) # (batch_size, sequence_length, vocab_size)\n",
      "\n",
      "Sample Indices: [2]\n",
      "--------------------------------------------------------\n",
      "Input: tf.Tensor(\n",
      "[[[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1.]]], shape=(1, 6, 9), dtype=float64)\n",
      "\n",
      "Target: tf.Tensor(\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0.]]], shape=(1, 6, 9), dtype=float64)\n",
      "\n",
      "Batche Predictions: tf.Tensor(\n",
      "[[ 0.01447803 -0.00242894 -0.01396178  0.02150395  0.01190736 -0.00065227\n",
      "   0.01840041 -0.00205957  0.00655539]], shape=(1, 9), dtype=float32) # (batch_size, sequence_length, vocab_size)\n",
      "\n",
      "Sample Indices: [8]\n",
      "--------------------------------------------------------\n",
      "Input: tf.Tensor(\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1.]]], shape=(1, 6, 9), dtype=float64)\n",
      "\n",
      "Target: tf.Tensor(\n",
      "[[[0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0.]]], shape=(1, 6, 9), dtype=float64)\n",
      "\n",
      "Batche Predictions: tf.Tensor(\n",
      "[[ 0.01487631 -0.00326066 -0.01524682  0.01957473  0.0114446  -0.0028499\n",
      "   0.01731911 -0.00388029  0.00893241]], shape=(1, 9), dtype=float32) # (batch_size, sequence_length, vocab_size)\n",
      "\n",
      "Sample Indices: [3]\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "##### Testing the model #####\n",
    "\n",
    "for input_example_batch, target_example_batch in datasetBatch:\n",
    "    print('Input:',input_example_batch)\n",
    "    print()\n",
    "    \n",
    "    print('Target:',target_example_batch)\n",
    "    print()\n",
    "    \n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print('Batche Predictions:',example_batch_predictions, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "    print()\n",
    "    \n",
    "    sampled_indices = tf.random.categorical(example_batch_predictions, num_samples=1)\n",
    "    sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "    print('Sample Indices:',sampled_indices)\n",
    "    print('--------------------------------------------------------')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.01447803 -0.00242894 -0.01396178  0.02150395  0.01190736 -0.00065227\n",
      "   0.01840041 -0.00205957  0.00655539]], shape=(1, 9), dtype=float32)\n",
      "[8]\n"
     ]
    }
   ],
   "source": [
    "##### Try prediction for the first example in the batch #####\n",
    "\n",
    "#print(example_batch_predictions)\n",
    "#sampled_indices = tf.random.categorical(example_batch_predictions, num_samples=1)\n",
    "#sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "#print(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
